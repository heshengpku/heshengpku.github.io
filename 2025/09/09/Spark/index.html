<!DOCTYPE html>
<html lang="zh-CN,en,default">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"heshengpku.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文整理了 Spark 的核心机制，涵盖 RDD 的设计思想、存储方式、不可变性、Stage 概念，以及 Spark 相对于 MapReduce 的改进。 来源于 Spark 最核心的两篇学术论文：  Zaharia, M., et al. (2012). Resilient Distributed Datasets: A Fault-Tolerant Abstraction for">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark技术原理">
<meta property="og:url" content="https://heshengpku.github.io/2025/09/09/Spark/index.html">
<meta property="og:site_name" content="何晟">
<meta property="og:description" content="本文整理了 Spark 的核心机制，涵盖 RDD 的设计思想、存储方式、不可变性、Stage 概念，以及 Spark 相对于 MapReduce 的改进。 来源于 Spark 最核心的两篇学术论文：  Zaharia, M., et al. (2012). Resilient Distributed Datasets: A Fault-Tolerant Abstraction for">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-09-09T07:07:26.000Z">
<meta property="article:modified_time" content="2025-09-09T08:18:52.058Z">
<meta property="article:author" content="He, Sheng">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="RDD">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://heshengpku.github.io/2025/09/09/Spark/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Spark技术原理 | 何晟</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">何晟</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">HE, SEHNG</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>公益 404</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://heshengpku.github.io/2025/09/09/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="He, Sheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="何晟">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark技术原理
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-09 15:07:26 / 修改时间：16:18:52" itemprop="dateCreated datePublished" datetime="2025-09-09T15:07:26+08:00">2025-09-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tools/" itemprop="url" rel="index"><span itemprop="name">Tools</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文整理了 Spark 的核心机制，涵盖 RDD
的设计思想、存储方式、不可变性、Stage 概念，以及 Spark 相对于 MapReduce
的改进。</p>
<p>来源于 Spark 最核心的两篇学术论文：</p>
<ol type="1">
<li><p><strong>Zaharia, M., et al. (2012). <em>Resilient Distributed
Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster
Computing</em>. NSDI.</strong> 这是 Spark 的“基础论文”，提出了 RDD
的概念。</p></li>
<li><p><strong>Zaharia, M., et al. (2010). <em>Spark: Cluster Computing
with Working Sets</em>. HotCloud.</strong> 这是 Spark
的早期论文，主要论证了基于内存的数据处理框架可以比 MapReduce
更快。</p></li>
</ol>
<span id="more"></span>
<h2 id="一spark-的技术原理">一、Spark 的技术原理</h2>
<p>在 Spark 出现之前，Hadoop MapReduce
已被广泛用于大规模批处理。但是在若干场景下，MapReduce
的开销和表达能力变成瓶颈：</p>
<ul>
<li><strong>迭代式算法昂贵</strong>：机器学习、图计算等需要多次扫描相同中间数据；MapReduce
每一步通常把中间结果写磁盘，I/O 成本极高。</li>
<li><strong>交互式分析迟滞</strong>：需要低延迟探索式查询时，MapReduce
高延迟不适合。</li>
<li><strong>编程表达力有限</strong>：MapReduce 编程模型主要围绕
map/reduce，两阶段模板不能自然表达复杂数据流。</li>
<li><strong>资源利用与调度粒度粗</strong>：每个作业开闭代价大，难以复用内存中已生成的数据。</li>
</ul>
<h3 id="核心抽象rddresilient-distributed-dataset">1.
核心抽象：RDD（Resilient Distributed Dataset）</h3>
<ul>
<li><p><strong>分布式集合</strong>：RDD
是一个不可变、分区化的数据集合，可以分布在集群不同节点上。</p></li>
<li><p><strong>容错机制</strong>：RDD
不依赖于数据复制（replication）来实现容错，而是通过
<strong>lineage（血缘记录）</strong>。即每个 RDD
都知道自己是如何由上游数据转换而来的，一旦分区丢失，可以通过 lineage
重新计算。</p></li>
<li><p><strong>操作</strong>分为两类：</p>
<ul>
<li><em>Transformations</em>（转换）：如
<code>map</code>、<code>filter</code>、<code>join</code>，延迟执行（lazy
evaluation）。</li>
<li><em>Actions</em>（动作）：如
<code>count</code>、<code>collect</code>、<code>save</code>，触发计算。</li>
</ul></li>
</ul>
<h3 id="内存计算">2. 内存计算</h3>
<ul>
<li>Spark 在执行时尽量将中间结果保存在 <strong>内存</strong>
中，避免频繁的磁盘读写。</li>
<li>对迭代式计算（机器学习、图计算）性能优势明显，因为可以反复访问缓存数据。</li>
</ul>
<h3 id="调度与执行">3. 调度与执行</h3>
<ul>
<li>Spark 用 <strong>有向无环图（DAG）</strong> 表示计算流程。</li>
<li>与 MapReduce 的两个阶段（map → reduce）不同，Spark 的 DAG
可以包含多个转换，系统会根据数据依赖关系自动划分 stage 并优化调度。</li>
</ul>
<h2 id="二rdd是什么">二、RDD是什么？</h2>
<ul>
<li><strong>逻辑数据集</strong>：RDD
表示一个不可变、分区化的分布式数据集合（logical
view），不是传统意义上总是把数据驻留在某个全局容器中。</li>
<li><strong>不可变（immutable）与纯函数式转换</strong>：所有转换（<code>map</code>、<code>filter</code>
等）产生新的 RDD，不修改原 RDD。</li>
<li><strong>延迟求值</strong>：transformations 是惰性的，action（如
<code>count</code>、<code>collect</code>）触发计算。</li>
<li><strong>lineage（血缘）</strong>：每个 RDD 保存如何从上游 RDD
生成自己的转换链（依赖信息），用于恢复和优化。</li>
<li><strong>StorageLevel</strong>：决定缓存策略（<code>MEMORY_ONLY</code>、<code>MEMORY_AND_DISK</code>、序列化等）。</li>
</ul>
<h3 id="抽象层面rdd-不是存数据的容器">1. 抽象层面：RDD
不是“存数据”的容器</h3>
<ul>
<li><p>RDD（Resilient Distributed
Dataset）这个名字容易让人误解，以为它像数据库表那样真正保存数据。</p></li>
<li><p><strong>实际上，RDD 更像是“数据的逻辑视图 +
元数据”</strong>，它自己并不直接保存数据，而是：</p>
<ul>
<li>保存数据的<strong>分区信息</strong>（partitions），</li>
<li>记录如何从上游数据计算得到这些分区的<strong>依赖关系（dependencies）</strong>
和 <strong>计算逻辑（compute 方法）</strong>，</li>
<li>保存数据在集群上的<strong>存储策略</strong>（storage
level：内存、磁盘、序列化等）。</li>
</ul></li>
</ul>
<p>换句话说：</p>
<blockquote>
<p>RDD = 数据分区描述 + lineage 依赖 + 计算方法 + 存储级别。</p>
</blockquote>
<h3 id="rdd-内部的数据结构源码角度">2. RDD
内部的数据结构（源码角度）</h3>
<p>在 Spark 源码中，RDD
是一个抽象类（<code>org.apache.spark.rdd.RDD[T]</code>），核心字段有：</p>
<ul>
<li><p><strong>partitions: Array[Partition]</strong></p>
<ul>
<li>描述 RDD 拥有多少个分区，每个分区是一个 <code>Partition</code>
对象（记录分区 ID、数据块的位置信息）。</li>
<li>分区是最小的调度单位。</li>
</ul></li>
<li><p><strong>dependencies: Seq[Dependency[_]]</strong></p>
<ul>
<li>记录 RDD 之间的依赖关系（窄依赖 / 宽依赖）。</li>
<li>这是 lineage 的核心，用来在容错时回溯。</li>
</ul></li>
<li><p><strong>compute(partition: Partition, context: TaskContext):
Iterator[T]</strong></p>
<ul>
<li>每个 RDD 子类都必须实现 <code>compute</code>
方法，定义如何计算某个分区的数据。</li>
<li>真正的数据不是提前存好的，而是 <strong>在 action 触发时，才通过
compute 动态计算生成</strong>。</li>
</ul></li>
<li><p><strong>storageLevel: StorageLevel</strong></p>
<ul>
<li>定义 RDD
的持久化策略（<code>MEMORY_ONLY</code>、<code>MEMORY_AND_DISK</code>、<code>DISK_ONLY</code>、序列化等）。</li>
<li>当用户调用 <code>persist()</code> 或 <code>cache()</code> 时，Spark
会把计算得到的数据缓存在内存（或磁盘）。</li>
</ul></li>
<li><p><strong>preferredLocations(partition: Partition):
Seq[String]</strong></p>
<ul>
<li>指示数据分区的最佳执行位置（数据本地性），帮助 Spark
调度时减少网络传输。</li>
</ul></li>
</ul>
<p>常见 RDD 子类（举例，有助理解数据来源）：</p>
<ul>
<li><code>HadoopRDD</code> / <code>NewHadoopRDD</code>：从 HDFS 或
Hadoop 输入格式读入。</li>
<li><code>MapPartitionsRDD</code> / <code>MappedRDD</code>：表示 map 或
mapPartitions 的结果（窄依赖）。</li>
<li><code>ShuffledRDD</code>：需要 shuffle 的中间结果（宽依赖）。</li>
<li><code>UnionRDD</code>、<code>FilteredRDD</code> 等。</li>
</ul>
<h3 id="存储时的物理形态">3. 存储时的物理形态</h3>
<ul>
<li><strong>默认</strong>：RDD
只是保持计算逻辑与分区描述，数据是在执行时通过 <code>compute</code>
生成。</li>
<li><strong>cache/persist</strong>：当用户 <code>cache()</code> 或
<code>persist()</code>，计算出的分区数据会被存储在每个 Executor 的
<strong>BlockManager</strong> 中。BlockManager
管理内存块、磁盘块（和序列化字节数组）并与 Master 协调位置元数据。</li>
</ul>
<p>当 RDD 被实际计算并缓存时，它的数据会以 <strong>JVM 对象</strong> 或
<strong>序列化字节数组</strong> 的形式存储：</p>
<ul>
<li><p><strong>MEMORY_ONLY</strong></p>
<ul>
<li>数据直接存成 JVM 对象（如 Scala 的 Array、Java 的对象）。</li>
<li>速度最快，但占用内存大。</li>
</ul></li>
<li><p><strong>MEMORY_AND_DISK</strong></p>
<ul>
<li>内存放不下的部分会溢写到磁盘，下次访问时从磁盘加载。</li>
</ul></li>
<li><p><strong>DISK_ONLY</strong></p>
<ul>
<li>全部存到磁盘。</li>
</ul></li>
<li><p><strong>MEMORY_ONLY_SER / MEMORY_AND_DISK_SER</strong></p>
<ul>
<li>数据序列化成字节数组存储，节省空间但 CPU 开销更大。</li>
</ul></li>
</ul>
<p>存储格式与 trade-off ：</p>
<ul>
<li><strong>JVM 对象（MEMORY_ONLY）</strong>：读取快，但占内存高，GC
成本需注意。</li>
<li><strong>序列化存储（MEMORY_ONLY_SER）</strong>：节省内存、提高网络传输效率，但序列化/反序列化消耗
CPU。推荐使用高效序列化（如 Kryo）。</li>
<li><strong>内存 +
磁盘（MEMORY_AND_DISK）</strong>：内存不足则溢出到磁盘。</li>
<li><strong>落盘（DISK_ONLY）</strong>：适用于内存非常紧张或极大数据。</li>
</ul>
<h3 id="小例子">4. 小例子</h3>
<p>假设有一段代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;hdfs://data/log.txt&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd1.map(_.split(<span class="string">&quot; &quot;</span>)(<span class="number">0</span>))   <span class="comment">// 取出第一列</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd2.distinct()</span><br><span class="line">rdd3.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>rdd1</code>：一个 <strong>HadoopRDD</strong>，分区信息来自
HDFS block。</li>
<li><code>rdd2</code>：一个 <strong>MappedRDD</strong>，保存了对
<code>rdd1</code> 的窄依赖，以及 <code>map</code> 的函数逻辑。</li>
<li><code>rdd3</code>：一个 <strong>DistinctRDD</strong>，依赖
<code>rdd2</code>（宽依赖）。</li>
<li><code>persist</code>：意味着当 <code>rdd3</code>
第一次计算出来时，会存到内存，不够就写磁盘。</li>
</ul>
<p>✅ <strong>总结一句话</strong>：</p>
<ul>
<li><strong>逻辑层面</strong>：RDD
是不可变的、分区化的逻辑数据集，核心由分区（partitions）、依赖（dependencies）、计算方法（compute）、存储策略（storage
level）组成。</li>
<li><strong>物理层面</strong>：RDD
本身不存数据，数据要么来自上游数据源（如 HDFS），要么由 lineage
计算得出；如果调用
<code>cache/persist</code>，才会把数据真正存到内存/磁盘。</li>
</ul>
<h3 id="什么叫不可变">5. 什么叫“不可变”？</h3>
<p><strong>RDD 的不可变性（immutability）</strong>
不是随便加的一个约束，而是 Spark 设计中至关重要的原则。</p>
<ul>
<li>当你对一个 RDD 做操作（比如
<code>map</code>、<code>filter</code>），你并不会在原有的 RDD
上修改数据。</li>
<li>相反，会生成一个新的 RDD（带有 lineage，指向上游 RDD
和转换函数）。</li>
<li>原始 RDD 始终保持不变。</li>
</ul>
<p>👉
类似函数式编程里的“不可变对象”，所有的转换操作都是“生成新对象”。</p>
<h4 id="简化容错机制">（1）简化容错机制</h4>
<ul>
<li><p>如果 RDD
是可变的，每个节点可能会以不同方式修改它，一旦出错就很难恢复原始状态。</p></li>
<li><p>由于 RDD 是不可变的，Spark 可以放心地只存 lineage：</p>
<ul>
<li>出错时，重新从最初数据 + 操作链计算，结果必然一致。</li>
</ul></li>
<li><p>这让 <strong>lineage 容错机制</strong> 成为可能。</p></li>
</ul>
<h4 id="简化并行计算">（2）简化并行计算</h4>
<ul>
<li>在分布式环境下，多个 task 可能同时访问同一个 RDD。</li>
<li>不可变性保证了不会有写冲突（write conflict）。</li>
<li>不需要加锁或复杂的同步机制，降低了并发编程的难度。</li>
</ul>
<h4 id="提升调度与优化的可预测性">（3）提升调度与优化的可预测性</h4>
<ul>
<li>因为 RDD 的内容不会被随意修改，Spark 调度器可以对整个 DAG
进行全局优化。</li>
<li>每个 stage
的依赖关系是确定的、纯函数式的，这让任务划分、数据本地性调度、重算等更可控。</li>
</ul>
<h4
id="更安全的缓存cachepersist">（4）更安全的缓存（Cache/Persist）</h4>
<ul>
<li>如果 RDD
是可变的，缓存的数据可能随时被别的任务修改，导致结果不一致。</li>
<li>不可变性保证了：一旦缓存，数据就不会变，后续访问结果始终一致。</li>
<li>这就是 Spark <code>cache()</code> / <code>persist()</code>
可以放心使用的根本原因。</li>
</ul>
<h4 id="函数式编程思想的延续">（5）函数式编程思想的延续</h4>
<ul>
<li>Spark 的 API 设计深受 Scala 和函数式编程影响。</li>
<li>不可变数据结构（immutable collections）+ 纯函数（map、filter） →
提供声明式编程模型。</li>
<li>开发者只描述“做什么”，而不用关心“怎么执行”，更容易表达复杂的数据流逻辑。</li>
</ul>
<h3 id="rdd-的局限与-spark-演进dataframe-dataset">6. RDD 的局限与 Spark
演进（DataFrame / Dataset）</h3>
<ul>
<li><strong>表达能力与优化空间</strong>：RDD
提供自由度高但缺乏全局语义。Spark 后来引入 DataFrame/Dataset + Catalyst
优化器与 Tungsten 执行层，在
SQL/结构化数据上能做更深度的算子融合、物理计划优化、代码生成，从而获得更好性能。
<ul>
<li>DataFrame：一张“分布式表”，类似数据库表，行列结构清晰，每列有数据类型。</li>
<li>Dataset：在 DataFrame
的基础上，提供了编译时类型检查和更强的面向对象支持。</li>
<li>这两者让 Spark 可以对“表结构”和“查询计划”做更高级的优化（不像 RDD
只是黑盒函数）。</li>
</ul></li>
<li><strong>什么时候还用
RDD？</strong>：低层控制（自定义序列化、精细分区策略、复杂非结构化流处理）或学习/研究原理时仍然有价值。但生产
ETL/SQL/ML 工作流推荐首选 DataFrame / Dataset。</li>
</ul>
<p>Catalyst 是 Spark SQL 的
<strong>查询优化器</strong>，它的作用类似数据库里的优化器。工作分四层：</p>
<ul>
<li><strong>解析</strong>：把 SQL / DataFrame
操作翻译成逻辑计划（Logical Plan）。</li>
<li><strong>规则优化</strong>：应用一系列规则（比如常量折叠、谓词下推、投影剪裁、算子融合），得到更优的逻辑计划。</li>
<li><strong>物理计划生成</strong>：根据数据大小和集群情况选择具体算子实现，比如选择
broadcast join 还是 shuffle join。</li>
<li><strong>代码生成（Codegen）</strong>：将最终的物理计划转化为高效的
JVM 字节码，避免解释执行带来的函数调用开销。</li>
</ul>
<p>这样，Spark 就能像数据库一样，自动为 SQL/DataFrame
查询生成更高效的执行方式。</p>
<p>Catalyst 负责“计划优化”，而 <strong>Tungsten
专注于“执行效率”</strong>：</p>
<ul>
<li><strong>内存管理优化</strong>：绕过 JVM 的对象存储和
GC，使用二进制格式直接管理内存。</li>
<li><strong>缓存效率</strong>：利用 CPU Cache
Line，压缩存储，减少内存访问开销。</li>
<li><strong>向量化执行</strong>：批量处理数据，减少函数调用开销。</li>
<li><strong>代码生成（Whole-Stage
Codegen）</strong>：把一系列算子融合成一段连续的 Java
方法，避免频繁的函数切换（类似手工写的 for 循环一样高效）。</li>
</ul>
<h2 id="三基于lineage实现容错机制">三、基于Lineage实现容错机制</h2>
<ul>
<li>RDD 是不可变的，且对每个 RDD 保存生成它的依赖链（lineage）。</li>
<li>当某个分区数据丢失（executor 崩溃、磁盘坏块等），Spark
不依赖副本去恢复，而是根据 lineage
重新计算该分区所需的上游分区并重放计算。</li>
</ul>
<h3 id="传统方式的容错问题">1. 传统方式的容错问题</h3>
<p>在分布式系统里，节点和数据分区经常会丢失。两种常见的容错手段是：</p>
<ul>
<li><strong>复制（Replication）</strong>：比如 HDFS，通常存 3
份副本，任何一份坏掉还能用别的副本。优点是恢复快，缺点是存储开销大（3
倍以上）。</li>
<li><strong>检查点（Checkpointing）</strong>：定期把中间状态保存下来，出错时从检查点恢复。优点是安全，缺点是频繁
I/O 成本高。</li>
</ul>
<p>MapReduce 主要依赖复制来保证容错，每一步计算的输出写入
HDFS，带来了大量的磁盘读写。</p>
<h3 id="spark-的改进lineage-血缘机制">2. Spark 的改进：Lineage
血缘机制</h3>
<p>Spark 提出了一种更轻量的思路：</p>
<ul>
<li><p><strong>RDD 是不可变的</strong>。当你对一个 RDD 做
<code>map</code> 或 <code>filter</code>
时，不会改变原数据，而是生成一个新的 RDD。</p></li>
<li><p><strong>RDD 记录“怎么来的”而不是“结果是什么”</strong>。</p>
<ul>
<li>每个 RDD 都保存了自己是由哪个上游 RDD，通过什么转换算子生成的（例如
<code>RDD2 = RDD1.map(f)</code>）。</li>
<li>这条“转换链”就是 <strong>lineage（血缘）</strong>。</li>
</ul></li>
<li><p><strong>容错时重算</strong>：</p>
<ul>
<li>如果某个分区丢失，Spark 不会去找副本，而是回溯
lineage，重新执行丢失分区所需的那部分计算。</li>
<li>因为 lineage
是逻辑依赖关系图（DAG），系统只需重算必要的分区，而不是整个数据集。</li>
</ul></li>
</ul>
<h3 id="原理举例">3. 原理举例</h3>
<p>假设有这样一条计算链：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RDD1 (HDFS 原始数据)</span><br><span class="line">   ↓ map</span><br><span class="line">RDD2</span><br><span class="line">   ↓ filter</span><br><span class="line">RDD3</span><br><span class="line">   ↓ reduceByKey</span><br><span class="line">RDD4 (最终结果)</span><br></pre></td></tr></table></figure>
<p>如果在计算 <code>RDD4</code> 时，某个分区丢了：</p>
<ol type="1">
<li>Spark 会查看 <code>RDD4</code> 的 lineage，发现它来自
<code>RDD3.reduceByKey</code>。</li>
<li>然后回溯到 <code>RDD3</code>，再发现它来自
<code>RDD2.filter</code>。</li>
<li>继续回溯到 <code>RDD2.map</code>，最后到
<code>RDD1</code>（原始数据还在 HDFS，有副本保障）。</li>
<li>Spark 就只需要重新从 <code>RDD1</code> 的相关分区开始，沿着 lineage
重新执行 map → filter → reduceByKey，恢复丢失的分区。</li>
</ol>
<blockquote>
<p>注意：只需
<strong>重算丢失的那几个分区</strong>，而不是整个数据集。</p>
</blockquote>
<h3 id="为什么高效">4. 为什么高效？</h3>
<ul>
<li><strong>省存储</strong>：不需要存 3 份副本，只要保存 lineage
信息即可。</li>
<li><strong>省 I/O</strong>：不用每步都落盘，只有在需要 checkpoint
或最终结果时才写磁盘。</li>
<li><strong>重算局部</strong>：出错时只重算丢失的分区，而不是整个任务。</li>
<li><strong>适合大数据场景</strong>：大多数大数据任务是“确定性转换”的（pure
function），lineage 保证了重算结果和原来一致。</li>
</ul>
<h3 id="特殊情况宽依赖和窄依赖">5. 特殊情况：宽依赖和窄依赖</h3>
<ul>
<li><strong>窄依赖（Narrow Dependency）</strong>：每个子 RDD
分区依赖于上游 RDD 的少数几个分区（如
<code>map</code>、<code>filter</code>），恢复时只需重算少量数据。</li>
<li><strong>宽依赖（Wide Dependency）</strong>：子 RDD
的每个分区依赖上游 RDD 的多个分区（如
<code>groupByKey</code>、<code>reduceByKey</code>），恢复时可能要重算较大范围数据。</li>
</ul>
<p>Lineage 很长或依赖中包含大量宽依赖时，重算代价会变高。Spark
提供两种缓解方式：</p>
<ul>
<li><strong>Checkpoint（检查点）</strong>：把 RDD 写入可靠存储（如
HDFS），并截断 lineage。适合长迭代或多阶段计算的中间结果持久化。</li>
<li><strong>缓存（persist）</strong>：把数据放进
BlockManager，避免每次都重复计算（不是容错的替代，但能减少重算频次）。</li>
</ul>
<p>✅ <strong>总结一句话</strong>： Spark 通过 <strong>lineage</strong>
记录 RDD 的生成逻辑，避免了昂贵的副本存储；一旦分区丢失，就沿着 lineage
重算必要的数据分区，从而实现了
<strong>轻量、高效的容错机制</strong>。</p>
<h2 id="四stage与task">四、Stage与Task</h2>
<p>用户通过 transformations 构建一棵有向无环图（DAG），节点是
RDD，边是依赖。Spark 的 DAGScheduler 会把这个逻辑 DAG
划分成若干物理阶段（stages）。</p>
<ul>
<li><strong>Stage</strong>：一段可以在不进行 shuffle
的情况下连续执行的转换链（由窄依赖组成）。遇到
<strong>宽依赖</strong>（需要 shuffle）时，Spark 会把 DAG
在该点切分为多个 Stage。</li>
<li>因此：多个连续的 <code>map</code>/<code>filter</code> 可以形成一个
Stage；带
<code>reduceByKey</code>、<code>groupByKey</code>、<code>join</code>
的节点会触发 shuffle，成为 Stage 边界。</li>
</ul>
<h3 id="rdd-转换-vs-stage">1. RDD 转换 vs Stage</h3>
<ul>
<li><p><strong>RDD 转换（transformation）</strong></p>
<ul>
<li>比如
<code>map</code>、<code>filter</code>、<code>reduceByKey</code>，每个转换都会生成一个新的
RDD。</li>
<li>转换只是逻辑上的描述（DAG 中的一个节点）。</li>
</ul></li>
<li><p><strong>Stage</strong></p>
<ul>
<li>是 Spark
执行调度的<strong>物理执行单元</strong>，由一组可以并行计算的任务（tasks）组成。</li>
<li>一个 Stage 对应于 DAG 中一段连续的、<strong>只有窄依赖（narrow
dependency）</strong> 的 RDD 转换链。</li>
<li>当遇到 <strong>宽依赖（wide dependency）</strong> 时（例如
<code>reduceByKey</code>、<code>groupByKey</code>、<code>join</code>），Spark
必须执行 <strong>shuffle</strong>，这会触发 Stage 的划分。</li>
</ul></li>
</ul>
<p>👉 换句话说： <strong>RDD 转换是逻辑层面的数据依赖，Stage
是物理层面的执行划分。</strong></p>
<h3 id="stage-划分规则">2. Stage 划分规则</h3>
<p>Spark 根据 <strong>RDD 依赖关系</strong> 来划分 Stage：</p>
<ul>
<li><p><strong>窄依赖 (Narrow Dependency)</strong></p>
<ul>
<li>子 RDD 的每个分区依赖于父 RDD 的少数几个分区（比如
<code>map</code>、<code>filter</code>）。</li>
<li>这些操作可以在同一个 Stage 内完成。</li>
</ul></li>
<li><p><strong>宽依赖 (Wide Dependency)</strong></p>
<ul>
<li>子 RDD 的每个分区依赖父 RDD 的多个分区（比如
<code>reduceByKey</code>、<code>groupByKey</code>）。</li>
<li>必须经过 <strong>shuffle</strong>，不同节点之间需要交换数据。</li>
<li>发生 shuffle 的地方，就是 Stage 的边界。</li>
</ul></li>
</ul>
<h3 id="举个例子">3. 举个例子</h3>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;hdfs://data/log.txt&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = rdd1.map(_.split(<span class="string">&quot; &quot;</span>)(<span class="number">0</span>))      <span class="comment">// 窄依赖</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd2.filter(_.nonEmpty)        <span class="comment">// 窄依赖</span></span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd3.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> rdd5 = rdd4.reduceByKey(_ + _)        <span class="comment">// 宽依赖（触发 shuffle）</span></span><br><span class="line"><span class="keyword">val</span> rdd6 = rdd5.mapValues(_ / <span class="number">2</span>)          <span class="comment">// 窄依赖</span></span><br><span class="line">rdd6.saveAsTextFile(<span class="string">&quot;hdfs://data/out&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Stage
1</strong>：<code>rdd1 → rdd2 → rdd3 → rdd4</code>（全部是窄依赖，可以流水线式执行）</li>
<li><strong>Stage
2</strong>：<code>rdd5</code>（<code>reduceByKey</code> 引发
shuffle，需要新的 Stage）</li>
<li><strong>Stage 3</strong>：<code>rdd6</code> → 输出（窄依赖）</li>
</ul>
<p>所以最终 DAG 会被拆成 <strong>3 个 Stage</strong>。</p>
<h3 id="stage-和-task-的关系">4. Stage 和 Task 的关系</h3>
<ul>
<li><p><strong>Stage</strong>：一组逻辑上可以并行执行的任务集合。</p></li>
<li><p><strong>Task</strong>：在一个具体分区上的计算单元。</p>
<ul>
<li>一个 Stage 会被分解为多个 Task，每个 Task 处理一个分区的数据。</li>
<li>Task 在 Executor 上并行执行。Stage 的并行度等于参与 Stage
的分区数。</li>
</ul></li>
</ul>
<p>举例：</p>
<ul>
<li>如果 Stage 有 100 个分区，就会产生 100 个 Task，分布到不同的
Executor 上执行。</li>
</ul>
<h3 id="为什么-shuffle-代价昂贵">5 为什么 Shuffle 代价昂贵？</h3>
<p>Shuffle 涉及
<strong>网络传输</strong>（跨节点数据移动）、<strong>磁盘
I/O</strong>（map 端写本地文件，reduce
端读取）、<strong>序列化/反序列化</strong>、以及
<strong>合并/排序</strong>（sort-based
shuffle）。这些都显著增加延迟与资源消耗：</p>
<ul>
<li>使用 <code>reduceByKey</code>（在 map 端进行 combine）代替
<code>groupByKey</code>，减少数据发送量。</li>
<li>通过合理的 <code>partitionBy</code> 与自定义 Partitioner
控制数据分布（避免数据倾斜）。</li>
<li>对小表使用 <strong>广播 Join（broadcast join）</strong>，避免
shuffle 大表。</li>
<li>合理使用
<code>mapPartitions</code>、避免频繁创建对象来减少序列化开销。</li>
</ul>
<h2 id="五总结与-mapreduce-相比的改进">五、总结：与 MapReduce
相比的改进</h2>
<h3 id="数据存储与迭代计算">1. 数据存储与迭代计算</h3>
<ul>
<li><strong>MapReduce</strong>：每一步的中间结果都会写入
HDFS，后续步骤需要重新从磁盘加载。适合单次批处理，但对迭代算法（如
PageRank、K-means）效率低下。</li>
<li><strong>Spark</strong>：将中间结果保存在内存中（也可选择落盘），极大提高了迭代计算和交互式计算性能。</li>
</ul>
<h3 id="编程模型">2. 编程模型</h3>
<ul>
<li><strong>MapReduce</strong>：程序员必须围绕“map +
reduce”两种操作来设计任务，表达能力有限。</li>
<li><strong>Spark</strong>：提供了更丰富的 API，包括
<code>map</code>、<code>filter</code>、<code>reduceByKey</code>、<code>join</code>
等算子，并支持 DAG 计算图，编程抽象更自然。</li>
</ul>
<h3 id="容错机制">3. 容错机制</h3>
<ul>
<li><strong>MapReduce</strong>：通过数据复制（replication，通常是 3 份
HDFS 副本）保证容错，但带来存储开销。</li>
<li><strong>Spark</strong>：通过 lineage
记录计算逻辑，只需在故障时重算丢失分区，不必复制全部数据，存储和恢复更高效。</li>
</ul>
<h3 id="性能">4. 性能</h3>
<ul>
<li><strong>MapReduce</strong>：磁盘 I/O 成本高，延迟大。</li>
<li><strong>Spark</strong>：内存计算 + DAG
优化，大幅提升性能。在论文实验中，Spark 在迭代式任务上可比 Hadoop
MapReduce 快 10~100 倍。</li>
</ul>
<h3 id="应用场景">5. 应用场景</h3>
<ul>
<li><strong>MapReduce</strong>：适合大规模批处理，如日志分析、数据清洗。</li>
<li><strong>Spark</strong>：既能做批处理，又能做迭代计算、流处理（Spark
Streaming）、SQL 查询（Spark
SQL）、机器学习（MLlib）、图计算（GraphX），生态更丰富。</li>
</ul>
<h3 id="总结">6. 总结</h3>
<ul>
<li>Spark 的核心创新是 <strong>RDD 抽象</strong>，用 lineage
提供了高效的容错方式。</li>
<li>Spark 将 <strong>内存计算 + DAG 执行模型</strong>
引入分布式计算，大幅减少了磁盘 I/O 的开销。</li>
<li>相比 MapReduce，Spark 在
<strong>迭代计算、交互式分析、多场景支持</strong>
等方面有了质的提升。</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Spark/" rel="tag"><i class="fa fa-tag"></i> Spark</a>
              <a href="/tags/RDD/" rel="tag"><i class="fa fa-tag"></i> RDD</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/06/04/Ollama-DeepSeek-R1/" rel="prev" title="用Ollama本地部署DeepSeek-R1模型">
      <i class="fa fa-chevron-left"></i> 用Ollama本地部署DeepSeek-R1模型
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/12/08/SemanticLayer/" rel="next" title="语义层（Semantic Layer）">
      语义层（Semantic Layer） <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80spark-%E7%9A%84%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86"><span class="nav-number">1.</span> <span class="nav-text">一、Spark 的技术原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%8A%BD%E8%B1%A1rddresilient-distributed-dataset"><span class="nav-number">1.1.</span> <span class="nav-text">1.
核心抽象：RDD（Resilient Distributed Dataset）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97"><span class="nav-number">1.2.</span> <span class="nav-text">2. 内存计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B0%83%E5%BA%A6%E4%B8%8E%E6%89%A7%E8%A1%8C"><span class="nav-number">1.3.</span> <span class="nav-text">3. 调度与执行</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8Crdd%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">2.</span> <span class="nav-text">二、RDD是什么？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8A%BD%E8%B1%A1%E5%B1%82%E9%9D%A2rdd-%E4%B8%8D%E6%98%AF%E5%AD%98%E6%95%B0%E6%8D%AE%E7%9A%84%E5%AE%B9%E5%99%A8"><span class="nav-number">2.1.</span> <span class="nav-text">1. 抽象层面：RDD
不是“存数据”的容器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rdd-%E5%86%85%E9%83%A8%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6"><span class="nav-number">2.2.</span> <span class="nav-text">2. RDD
内部的数据结构（源码角度）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%98%E5%82%A8%E6%97%B6%E7%9A%84%E7%89%A9%E7%90%86%E5%BD%A2%E6%80%81"><span class="nav-number">2.3.</span> <span class="nav-text">3. 存储时的物理形态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E4%BE%8B%E5%AD%90"><span class="nav-number">2.4.</span> <span class="nav-text">4. 小例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E5%8F%AB%E4%B8%8D%E5%8F%AF%E5%8F%98"><span class="nav-number">2.5.</span> <span class="nav-text">5. 什么叫“不可变”？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8C%96%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6"><span class="nav-number">2.5.1.</span> <span class="nav-text">（1）简化容错机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8C%96%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="nav-number">2.5.2.</span> <span class="nav-text">（2）简化并行计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8F%90%E5%8D%87%E8%B0%83%E5%BA%A6%E4%B8%8E%E4%BC%98%E5%8C%96%E7%9A%84%E5%8F%AF%E9%A2%84%E6%B5%8B%E6%80%A7"><span class="nav-number">2.5.3.</span> <span class="nav-text">（3）提升调度与优化的可预测性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9B%B4%E5%AE%89%E5%85%A8%E7%9A%84%E7%BC%93%E5%AD%98cachepersist"><span class="nav-number">2.5.4.</span> <span class="nav-text">（4）更安全的缓存（Cache&#x2F;Persist）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E6%80%9D%E6%83%B3%E7%9A%84%E5%BB%B6%E7%BB%AD"><span class="nav-number">2.5.5.</span> <span class="nav-text">（5）函数式编程思想的延续</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rdd-%E7%9A%84%E5%B1%80%E9%99%90%E4%B8%8E-spark-%E6%BC%94%E8%BF%9Bdataframe-dataset"><span class="nav-number">2.6.</span> <span class="nav-text">6. RDD 的局限与 Spark
演进（DataFrame &#x2F; Dataset）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E5%9F%BA%E4%BA%8Elineage%E5%AE%9E%E7%8E%B0%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6"><span class="nav-number">3.</span> <span class="nav-text">三、基于Lineage实现容错机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E6%96%B9%E5%BC%8F%E7%9A%84%E5%AE%B9%E9%94%99%E9%97%AE%E9%A2%98"><span class="nav-number">3.1.</span> <span class="nav-text">1. 传统方式的容错问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-%E7%9A%84%E6%94%B9%E8%BF%9Blineage-%E8%A1%80%E7%BC%98%E6%9C%BA%E5%88%B6"><span class="nav-number">3.2.</span> <span class="nav-text">2. Spark 的改进：Lineage
血缘机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E7%90%86%E4%B8%BE%E4%BE%8B"><span class="nav-number">3.3.</span> <span class="nav-text">3. 原理举例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%AB%98%E6%95%88"><span class="nav-number">3.4.</span> <span class="nav-text">4. 为什么高效？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E6%AE%8A%E6%83%85%E5%86%B5%E5%AE%BD%E4%BE%9D%E8%B5%96%E5%92%8C%E7%AA%84%E4%BE%9D%E8%B5%96"><span class="nav-number">3.5.</span> <span class="nav-text">5. 特殊情况：宽依赖和窄依赖</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9Bstage%E4%B8%8Etask"><span class="nav-number">4.</span> <span class="nav-text">四、Stage与Task</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#rdd-%E8%BD%AC%E6%8D%A2-vs-stage"><span class="nav-number">4.1.</span> <span class="nav-text">1. RDD 转换 vs Stage</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#stage-%E5%88%92%E5%88%86%E8%A7%84%E5%88%99"><span class="nav-number">4.2.</span> <span class="nav-text">2. Stage 划分规则</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BE%E4%B8%AA%E4%BE%8B%E5%AD%90"><span class="nav-number">4.3.</span> <span class="nav-text">3. 举个例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#stage-%E5%92%8C-task-%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">4.4.</span> <span class="nav-text">4. Stage 和 Task 的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-shuffle-%E4%BB%A3%E4%BB%B7%E6%98%82%E8%B4%B5"><span class="nav-number">4.5.</span> <span class="nav-text">5 为什么 Shuffle 代价昂贵？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E6%80%BB%E7%BB%93%E4%B8%8E-mapreduce-%E7%9B%B8%E6%AF%94%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="nav-number">5.</span> <span class="nav-text">五、总结：与 MapReduce
相比的改进</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E4%B8%8E%E8%BF%AD%E4%BB%A3%E8%AE%A1%E7%AE%97"><span class="nav-number">5.1.</span> <span class="nav-text">1. 数据存储与迭代计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.2.</span> <span class="nav-text">2. 编程模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6"><span class="nav-number">5.3.</span> <span class="nav-text">3. 容错机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD"><span class="nav-number">5.4.</span> <span class="nav-text">4. 性能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">5.5.</span> <span class="nav-text">5. 应用场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">5.6.</span> <span class="nav-text">6. 总结</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="He, Sheng"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">He, Sheng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/heshengpku" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;heshengpku" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:heshengpku@gmail.com" title="E-Mail → mailto:heshengpku@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?user=2BKQWbIAAAAJ" title="Google → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?user&#x3D;2BKQWbIAAAAJ" rel="noopener" target="_blank"><i class="fa fa-fw fa-google"></i>Google</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/heshengpku/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;heshengpku&#x2F;" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin"></i>Linkedin</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://orcid.org/0000-0002-7142-8990" title="ORCiD → https:&#x2F;&#x2F;orcid.org&#x2F;0000-0002-7142-8990" rel="noopener" target="_blank"><i class="fa fa-fw fa-google"></i>ORCiD</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/sheng-he-14" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;sheng-he-14" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin"></i>Zhihu</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        
  <div class="languages">
    <label class="lang-select-label">
      <i class="fa fa-language"></i>
      <span>简体中文</span>
      <i class="fa fa-angle-up" aria-hidden="true"></i>
    </label>
    <select class="lang-select" data-canonical="">
      
        <option value="zh-CN" data-href="/2025/09/09/Spark/" selected="">
          简体中文
        </option>
      
        <option value="en" data-href="/en/2025/09/09/Spark/" selected="">
          English
        </option>
      
    </select>
  </div>

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">He, Sheng</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">115k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:45</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
